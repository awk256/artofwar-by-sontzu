
# 概要

孫子の「五事」に基づいた社内AI原則は、単なるマナー集ではありません。それは、組織がAIという強力なリソースを運用する際の　**「リスク管理と勝利の最低条件」**　を明文化したものです。

# 社内AI原則（AI Governance Principles）

### 1. AIは目的を限定して使う（道 | Why）

* **原則：** AIの導入は、明確な業務目的および定量的な価値改善が定義されている場合に限ります。
* **指針：** 「AIを使ってみたい」という技術的好奇心のみによる本番導入を禁止し、常にビジネス上の「大義（Why）」を優先します。

### 2. AIの判断は必ず人が引き受ける（将 | Who）

* **原則：** すべてのAIサービスには、判断の結果に対して最終的な責任を負う「責任者」と、即座の「停止権限者」を任命しなければなりません。
* **指針：** 「AIが判断したから」という理由は、いかなる場合も免責事項として認められません。

### 3. 時期尚早なAIは使わない（天 | When）

* **原則：** 精度、コスト、法規制、および社会的な受容性が実用域に達していない技術は、本番環境への導入を認めません。
* **指針：** 市場の過熱感（ブーム）に流されず、技術的成熟度を冷徹に評価（計）した上で導入時期を決定します。

### 4. 現場とデータに合わないAIは作らない（地 | Where）

* **原則：** 自社の保有データ、実際の業務フロー、および現場の運用体制に適合しないAIモデルの導入は行いません。
* **指針：** 「汎用的な凄さ」よりも、自社のフィールド（地）における「実効性」を最優先の評価軸とします。

### 5. 止められないAIは不合格（法 | How）

* **原則：** [Human-in-the-loop](https://www.google.com/search?q=020-032.md)、操作ログ、判断の再現性、および緊急停止手段を持たないAIシステムを運用してはなりません。
* **指針：** 規律（法）なき自動化は「暴走」と定義し、監視・介入の仕組みがない設計はすべて不合格とします。

>[020-032.Human-in-the-loop（HITL）：将の介在設計](020-032.Human-in-the-loop（HITL）：将の介在設計.md)

#### 6. AIは社会的に使ってよいものであること（倫理 | 補正）

* **原則：** 公益性、公平性、説明可能性、および透明性を満たさないAIサービスは導入しません。
* **指針：** 孫子の合理性に「現代の倫理パッチ」を当て、社会に害を及ぼす可能性のある技術活用を未然に防ぎます。

# 社内原則・一文要約

> **AIは「賢いか」よりも、**
> **「制御でき、説明でき、止められるか」で判断する。**

この原則を遵守することは、短期的には導入のブレーキに見えるかもしれません。しかし、長期的には「取り返しのつかない大敗」を防ぎ、組織が持続的にAIの果実を得るための **「不敗の形（軍形）」** を構築することに繋がります。
