
# 概要

**Human-in-the-loop（HITL）とは、AIという「兵」に判断の全権を委ねず、人間という「将」が決定の連鎖に留まり続けるための設計思想です。** 孫子は、状況の変化に応じられない固定的な作戦（自動化）を「敗道」と呼びました。HITLの本質は、AIの効率性を活かしつつ、決定的な場面で「将の知性（倫理・責任・文脈）」をどう割り込ませるかという、高度なインターフェース設計にあります。

# HITL：人間が介在すべき「距離」の自問

システムの性質と「負けた時の被害」の大きさに応じて、人間が介在する距離を設計します。

| パターン                  | 孫子的解釈              | 思考のチェックポイント              |
| --------------------- | ------------------ | ------------------------ |
| **Human-in-the-loop** | **「将、命を受けて軍を聚める」** | **「一歩も間違えられない戦いか？」**<br> |

<br>AIの提案ごとに人間が「承認」する。判断の責任が100%人間に残る。 |
| **Human-on-the-loop** | **「将、軍を指揮して敵を制す」** | **「大局は任せられるが、油断できないか？」**<br>

<br>AIは自律稼働するが、人間が「常時監視」し、異常時にのみ即時介入する。 |
| **Human-over-the-loop** | **「将、道の不可なるを知る」** | **「ルールそのものが変わる可能性はあるか？」**<br>

<br>通常はAIに任せ、人間は「方針の変更」や「システムの停止」のみを司る。 |


# なぜ「全自動」ではなく「HITL」なのか：3つの問い

「自動化率100%」という目標が、かえって組織を脆弱にしていないかを自問するための視点です。

### 1. 破壊的力の制御：火攻めの自滅を避ける

孫子は「火攻（破壊的な力）」は制御手段があって初めて機能すると説きました。AIの誤判断という「火」が組織を焼き尽くす前に、人間が消火器（介入権限）を持ってループ内に留まっているか？

### 2. 文脈の補完：信（一貫性）の担保

AIのアウトプットは確率的であり、時に「空気を読まない」判断をします。人間が介在し、一貫性のある最終判断を下すことで、社会からの「信（信頼）」を維持できているか？

### 3. 責任の所在：将の不敗の形

「AIが勝手にやった」という弁明は、孫子の世界では通用しません。人間がループに入ることで、判断の責任をアルゴリズムから人間へと引き戻し、ガバナンスという名の「不敗の形」を整えられているか？


# 具体的実装への示唆：九変・用間・法

* **「退く勇気」の設計（九変）：** AIの確信度が低い際、プライド（自動化への固執）を捨てて即座に人間へ判断を仰ぐ「エスカレーション・パス」はあるか。
* **「戦訓」の再学習（用間）：** 人間がAIの誤りを修正し、それを「次の計（学習データ）」として活かすためのフィードバック・ループは機能しているか。
* **「規律」の発動（法）：** 異常を検知した際、システムの外側から「将」が介入し、物理的に停止させる権限と手段（キルスイッチ）が担保されているか。

# 一文まとめ

> **HITLの設計とは、単なる「確認作業」の追加ではない。**
> **AIに「何を任せ」、人間に「何を返すべきか」という、**
> **責任の境界線を、状況に合わせて引き続けるための「知的な戦陣」である。**

「全自動」への過度な期待は、往々にして「将（責任者）」の不在を招きます。適切な介在設計こそが、不確実なAIを確実な武器へと変えるための、現代における「将の器」の証明となるのです。
